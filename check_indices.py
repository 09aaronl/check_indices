#!/usr/bin/env python3

''' This script contains a number of utilities for checking indices
    in a sample sheet, prior to pooling.
    It uses a sample sheet format based on Broad Institute's Picard.
'''

__author__ = "Aaron Lin <aelin@princeton.edu>"
__version__ = "1.0"

import argparse
import pandas as pd
import numpy as np
from itertools import combinations
from collections import Counter
import sys

class FormatError(Exception):
    pass

def format_ss(sample_sheet):
    ''' Formats sample sheet, prior to analysis.
        Input: Path to the sample sheet. Header column names must be 'Index,percent' or 'Index,Index2,percent'.
        Output: Pandas dataframe, with merged index (if applicable) and percent of pool.
    '''

    # Import as DataFrame
    df_in = pd.read_csv(sample_sheet)

    # Check that the header is formatted properly
    if len(df_in.columns) == 2:
        # Expect a single-indexed sample sheet header
        if all(df_in.columns == ['Index', 'percent']) == False:
            raise FormatError('Sample sheet must have first column: Index,percent (single index) or Index,Index2,percent (dual index)')
    elif len(df_in.columns) == 3:
        # Expect a dual-indexed sample sheet header
        if all(df_in.columns == ['Index', 'Index2', 'percent']) == False:
            raise FormatError('Sample sheet must have first column: Index,percent (single index) or Index,Index2,percent (dual index)')
    else:
        # Wrong number of columns
        raise FormatError('Sample sheet must have first column: Index,percent (single index) or Index,Index2,percent (dual index)')

    # Create a new sample sheet to be passed on
    df = df_in.copy()

    # If dual index, combine to single index
    if 'Index2' in df.columns:
        df['Index'] = df['Index'] + df['Index2']
        df = df.drop(columns='Index2')

    # Convert to uppercase
    df['Index'] = df.apply(lambda row: row['Index'].upper(), axis=1)

    # Check for only ACTG bases
    df['valid_chars'] = df.apply(lambda row: all(bases in set('ACGT') for bases in row['Index']), axis=1)
    df_invalidchars = df[df['valid_chars'] == False].set_index('Index')
    if len(df_invalidchars) > 0:
        for x in df_invalidchars.index:
            print(df_in[df['Index'] == x])
        raise FormatError('Found invalid characters!')

    # Check for duplicate indices
    df_tmp = df['Index'].value_counts().rename_axis('unique_indices').reset_index(name='counts').set_index('unique_indices')
    df_dups = df_tmp[df_tmp['counts'] > 1]
    if len(df_dups > 0):
        for x in df_dups.index:
            print("Index:", x, ", counts:", df_dups['counts'][x])
            print(df_in[df['Index'] == x])
        raise FormatError('Found duplicate indices!')

    # Check that each index is same length
    df['length'] = [len(x) for x in df['Index']]
    df_tmp = df['length'].value_counts().rename_axis('unique_lengths').reset_index(name='counts').set_index('unique_lengths')
    if len(df_tmp) > 1:
        majority_len = df_tmp.index[0]
        print('Most common length:', majority_len, ", counts:", df_tmp.values[0,0])
        df_wronglen = df[df['length'] != majority_len]
        print(df_wronglen)
        raise FormatError('Found indices of different lengths!')

    # Check that values sum to ~100
    percent_sum = df['percent'].sum()
    if percent_sum > 105 or percent_sum < 95:
        print('Percent sum:', percent_sum)
        raise FormatError('Percent sums do not total ~100%')

    df = df.drop(columns=['length', 'valid_chars'])
    return df

def hamming_distance(ind1, ind2):
    ''' Calculates Hamming distance for a single pair of indices.
        Input: Two strings. Guaranteed to be the same length due to earlier call to format_ss().
        Output: Integer value of Hamming distance.
    '''

    # Hamming distance code from https://en.wikipedia.org/wiki/Hamming_distance#Algorithm_example
    return sum(base1 != base2 for base1, base2 in zip(ind1, ind2))

def hamming_pairwise(args):
    ''' Calculates pairwise Hamming distance for all indices in the sample sheet. Reports minimum distance(s) and any pairs below a user-defined threshold
        Input: Path to the sample sheet. Minimum Hamming distance threshold for reporting.
        Output: Creates/overwrites 'dsub-jobstatus.txt' and 'samples-total.txt' in the current working directory.
    '''

    # Import the input Sample Sheet as a DataFrame
    df = format_ss(args.SampleSheet)
    dist_min = int(args.min_distance)

    inds = df['Index'].tolist()
    n = len(inds)

    # Calculate pairwise hamming distances between all indices. Code partially generated by Google AI Overview
    dists = np.zeros((n, n), dtype=int)
    for i, j in combinations(range(n), 2):
        dist = hamming_distance(inds[i], inds[j])
        dists[i, j] = dists[j, i] = dist

    # Print warning to stdout if any Hamming distances are below the minimum threshold
    warns = np.where((dists < dist_min))
    if(len(warns[0]) == n): # Distances are 0 along the diagonal (each index with itself), ignore diagonal zeroes
        print('No index pairs had hamming distance <', dist_min)
    else:
        for i, j, in zip(warns[0], warns[1]):
            if i > j:
                print('Index pair', inds[i], inds[j], ': distance =', dists[i][j])

    dists = pd.DataFrame(dists)
    dists.columns = inds
    dists.index = inds

    # Print entire 2D Hamming distance matrix to output csv file
    dists.to_csv(args.output)
    return dists

def diversity(args):
    ''' Use pysam to read unmapped bam, store specific bases as read tags
        Creates a tab-separated file 'dsub-jobstatus.txt' that stores integer values corresponding to the job status of each sample and sample-seqrun (row) for each step of a pipeline (column).
        Also creates a file 'samples-total.txt' that stores the names of each sample-seqrun that will be used for steps of the pipeline.
        Integer values: 0 = not yet run or still running; 1 = "Success"; -1 = "Failure and do re-run". Manually set job status value to -2 = "Failure and/or don't re-run".
        Input: Name of the pipeline to be run. This will determine which steps (columns) are created in 'dsub-jobstatus.txt'.
        Additional input: 'flowcells.txt' must be in the current working directory.
        Output: Creates/overwrites 'dsub-jobstatus.txt' and 'samples-total.txt' in the current working directory.
    '''

    # Import the input Sample Sheet as a DataFrame
    df = format_ss(args.SampleSheet)
    div_max = float(args.max_monotemplate)

    bases = ['A', 'C', 'G', 'T']
    n = len(df['Index'][0])

    div = np.zeros((n, len(bases)), dtype=float)
    i = 0
    # Loop over each position
    while i < len(div):
        df['current_base'] = [x[i:i+1] for x in df['Index']]

        j = 0
        # Calculate the percentage of base, for each of the 4 bases
        for base in bases:
            div[i, j] = df[df['current_base'] == base]['percent'].sum()
            j += 1
        i += 1

    div = np.round(div, decimals=1)

    # Print warning to stdout if any base diversities are above the maximum threshold
    warns = np.where(div > div_max)
    if(len(warns[0]) == 0):
        print('No bases had monotemplate >', div_max)
    else:
        for i, j, in zip(warns[0], warns[1]):
            print('At position', i+1, ':', bases[j], '=', div[i, j])

    div = pd.DataFrame(div.transpose())
    div.columns = [x + 1 for x in div.columns.tolist()]
    div.index = bases

    # Print entire 2D base diversity matrix to output csv file
    div.to_csv(args.output)
    return div


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Tool for checking sample indices prior to library pooling.")

    subparsers = parser.add_subparsers()
    # Subparser for Hamming distance
    parser_ham = subparsers.add_parser('hamming')
    parser_ham.add_argument('SampleSheet', help="Path to sample sheet, e.g., '/path/to/SampleSheet.csv'. Header column names must be 'Index,percent' or 'Index,Index2,percent'.")
    parser_ham.add_argument('output', help="Path/name for output, e.g., '/path/to/output.csv'.")
    parser_ham.add_argument('-min', '--min_distance', help="Reports any pairs below min. Default: < %(default)s", default=3)
    parser_ham.set_defaults(func=hamming_pairwise)
    # Subparser for base diversity
    parser_div = subparsers.add_parser('diversity')
    parser_div.add_argument('SampleSheet', help="Path to sample sheet, e.g., '/path/to/SampleSheet.csv'. Header column names must be 'Index,percent' or 'Index,Index2,percent'.")
    parser_div.add_argument('output', help="Path/name for output, e.g., '/path/to/output.csv'.")
    parser_div.add_argument('-max', '--max_monotemplate', help="Reports any bases above max. Default: > %(default)s", default=60.0)
    parser_div.set_defaults(func=diversity)
    
    # Parse arguments
    args = parser.parse_args()
    args.func(args)
